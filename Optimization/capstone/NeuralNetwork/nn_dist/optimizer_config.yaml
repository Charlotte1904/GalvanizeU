#=========================================================================
# File name         : optimizer_config.yaml
# File Description  : Parameter & Hyperparameter Optimizer Configurations
# File Version      : 1.0
# Author            : Srini Ananthakrishnan
# Date created      : 04/28/2017
# Date last modified: 04/28/2017
#=========================================================================
#
# OPTIMIZER CONFIGURATION FILE
# File contains dictionary like items in yaml. Format of which is key: value(s)
# User may change value(s) here and sacred framework will reflect in running configuration
#
# Example Usage:
# python optimizer.py with optimizer_config.yaml
#
# ------------------------------------------
# Class object 'name' instantiated by sacred
# ------------------------------------------
trainer: Optimize         # Optimizer Class Name
hyperparam_opt: RS        # hyper parameter optimizer. Default RS: Random Search

# ---------------------------------
# Hyper Parameters For Optimization
# ---------------------------------
nn_dimensions:            # number of neural network nodes for [input, output]
- 4
- 1
hidden_layers:            # hidden_layer [min, max]
- 4
- 10
batch_size:               # batch size as [lower_bound, upper_bound]
- 100
- 1000
activation:               # activation for non-linearity. Default is 'Relu'. Supports 'tanh'
- Relu
- tanh
learning_rate:            # learning rate as [lower_bound, upper_bound]
- 0.001
- 0.0001
train_optimizer:          # optimizer to be used for train. Default is 'Adam'. Supports 'sgd', 'Adagrad'
- Adam
- sgd
- Adagrad
opt_tolerance: 1.0e-05    # outer optimizer loop loss convergence threshold
train_tolerance: 1.0e-08  # inner train loop loss convergence threshold
opt_epoch: 3              # optimizer epoch is the outer loop for optimizing hyperparameters
train_epoch: 1000         # training epoch is the inner loop for training input data

# ----------------------------------------
# Parameters for synthetic data generation
# Valid ONLY when load_data is false
# ----------------------------------------
add_cosine: false         # transforms Y to element wise cosine (used if power_method is True)
add_noise: false          # adds random noise to output Y (used if power_method is True)
power_method: false       # generate matrix polynomial of incremental "power" of (M). Used ONLY load_data is false
data_features: 4          # number of input data instances (N). Used ONLY load_data is false
data_instances: 10000     # number of input data instances (N). Used ONLY load_data is false

# ---------------------------------
# Directory for loading real data
# Valid ONLY when load_data is true
# ---------------------------------
load_data: false          # When 'true' Load real data(X) of shape (N, M) and labels(Y) of shape (N,). Otherwise generate synthetic data.
load_data_dir: /tmp/some_dir  # Real data directory (like Boston, Iris, etc.)

# ---------------------------------
# Logging Level and Directory
# ---------------------------------
train_log_dir: /tmp/nn_dist/train_logs
logging_level:
- critical
- error
- warning
- info
- debug
- notset

# ----------------------------------------------------
# TensorFlow Sync_Replicas (synchronized workers) mode
# In this mode the parameter updates from workers are
# aggregated before applied to avoid stale gradients
# ----------------------------------------------------
sync_replicas: false    # Use the sync_replicas (synchronized replicas) mode

# -------------------------------------
# GPU config
# -------------------------------------
num_gpus: 0             # Number of GPUs. If > 0 GPUs will be used by Worker. Otherwise CPUs.

# -------------------------------------
# Tensorflow Distributed Cluster Config
# Format:
# localhost:port_number (or)
# 193.168.xxx.xxx:port_number
# -------------------------------------
# Distribued
#ps_hosts: localhost:2223,localhost:2224 # parameter server config
#worker_hosts: localhost:2225,localhost:2226,localhost:2227,localhost:2228 # worker server config
# Simple
ps_hosts: localhost:2223 # parameter server config
worker_hosts: localhost:2225 # worker server config

# -------------------------------------
# File to be forked for distributed jobs
# -------------------------------------
file2distribute: nn_distributed.py