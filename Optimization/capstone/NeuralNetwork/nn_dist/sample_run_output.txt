File: Sample_Run_Output
Updated: 04/28/2017
Author: Srini Ananthakrishnan

Note: Look at ps_0.log and worker_0.log for training epoch logs (inner-loop)

Below is optimizer output (outer-loop)
=================
OPTIMIZER Output
=================

(tensorflow) Srinivasas-Air:nn_dist srianant$ python optimizer.py with optimizer_config.yaml
'Epoch [1] config:'
{'activation': 'Relu',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 651,
 'data_features': 4,
 'data_instances': 10000,
 'hidden_layer_bounds': [4, 10],
 'input_dim': 4,
 'learning_rate': 0.00040498285677880504,
 'load_data': False,
 'logging_level': 'info',
 'nodes_per_layer': [4, 7, 7, 4, 4, 4, 5, 9, 9, 1],
 'opt_epoch': 3,
 'opt_epoch_iter': 1,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223',
 'train_epoch': 1000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'sgd',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225'}
 START OF Optimizer EPOCH ====================>> [ 1 ]
Deleteing /tmp/nn_dist
Forking Worker/PS Cluster Jobs @  1493435700.688643
cmd: %s python nn_distributed.py --ps_hosts=localhost:2223 --worker_hosts=localhost:2225 --job_name=ps --task_index=0
cmd: %s python nn_distributed.py --ps_hosts=localhost:2223 --worker_hosts=localhost:2225 --job_name=worker --task_index=0
Waiting for Worker process to complete....
INFO:tensorflow:SyncReplicasV2: replicas_to_aggregate=1; total_num_replicas=1
INFO:tensorflow:global_step/sec: 0
poller flags: 16 14
All Worker Process DONE...!!!
Worker processing ends @  1493435790.118755
Worker processing elapsed time:  89.43011212348938  secs
Cleanup Worker/PS Cluster Jobs..
new_loss: 0.238770143062 best_loss: 0.238770143062
END OF Optimizer EPOCH =====================>>[ 1 ]

'Epoch [2] config:'
{'activation': 'tanh',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 897,
 'data_features': 4,
 'data_instances': 10000,
 'hidden_layer_bounds': [4, 10],
 'input_dim': 4,
 'learning_rate': 0.00025201250784043976,
 'load_data': False,
 'logging_level': 'info',
 'nodes_per_layer': [4, 9, 6, 8, 8, 6, 8, 5, 1],
 'opt_epoch': 3,
 'opt_epoch_iter': 2,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223',
 'train_epoch': 1000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adagrad',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225'}
 START OF Optimizer EPOCH ====================>> [ 2 ]
Deleteing /tmp/nn_dist
Forking Worker/PS Cluster Jobs @  1493435790.179285
cmd: %s python nn_distributed.py --ps_hosts=localhost:2223 --worker_hosts=localhost:2225 --job_name=ps --task_index=0
cmd: %s python nn_distributed.py --ps_hosts=localhost:2223 --worker_hosts=localhost:2225 --job_name=worker --task_index=0
Waiting for Worker process to complete....
INFO:tensorflow:SyncReplicasV2: replicas_to_aggregate=1; total_num_replicas=1
INFO:tensorflow:global_step/sec: 0
poller flags: 16 14
All Worker Process DONE...!!!
Worker processing ends @  1493435860.357282
Worker processing elapsed time:  70.17799687385559  secs
Cleanup Worker/PS Cluster Jobs..
new_loss: 2.08489019087 best_loss: 0.238770143062
END OF Optimizer EPOCH =====================>>[ 2 ]


'Epoch [3] config:'
{'activation': 'Relu',
 'add_cosine': False,
 'add_noise': False,
 'batch_size': 825,
 'data_features': 4,
 'data_instances': 10000,
 'hidden_layer_bounds': [4, 10],
 'input_dim': 4,
 'learning_rate': 0.0001980480347370466,
 'load_data': False,
 'logging_level': 'info',
 'nodes_per_layer': [4, 8, 7, 9, 8, 5, 8, 6, 6, 1],
 'opt_epoch': 3,
 'opt_epoch_iter': 3,
 'opt_tolerance': 1e-05,
 'output_dim': 1,
 'power_method': False,
 'ps_hosts': 'localhost:2223',
 'train_epoch': 1000,
 'train_log_dir': '/tmp/nn_dist/train_logs',
 'train_optimizer': 'Adagrad',
 'train_tolerance': 1e-08,
 'worker_hosts': 'localhost:2225'}
 START OF Optimizer EPOCH ====================>> [ 3 ]
Deleteing /tmp/nn_dist
Forking Worker/PS Cluster Jobs @  1493435860.415272
cmd: %s python nn_distributed.py --ps_hosts=localhost:2223 --worker_hosts=localhost:2225 --job_name=ps --task_index=0
cmd: %s python nn_distributed.py --ps_hosts=localhost:2223 --worker_hosts=localhost:2225 --job_name=worker --task_index=0
Waiting for Worker process to complete....
INFO:tensorflow:SyncReplicasV2: replicas_to_aggregate=1; total_num_replicas=1
INFO:tensorflow:global_step/sec: 0
poller flags: 16 14
All Worker Process DONE...!!!
Worker processing ends @  1493435930.778613
Worker processing elapsed time:  70.36334109306335  secs
Cleanup Worker/PS Cluster Jobs..
new_loss: 308.475412811 best_loss: 0.238770143062
END OF Optimizer EPOCH =====================>>[ 3 ]


=========================
Summary:
=========================
Total EPOCH runs: 3
FINAL LOSS......: 0.238770143062
=========================













===================
OLD Updates:
===================
Updated: 04/22/2017

Distributed RUN loss converged with approx 4200 epochs, compare to 17200 epocs on non-distribued.
Also, distributed took 60 sec less than non-distribued. This was done with loss tolerance less than 1e-8. 

=========================
With SINGLE Worker-0/PS-0
=========================
[1] worker/0 1492895302.338650: training step 17231 done (global step: 137855) with Loss 0.000862, 0.000862
[1] worker/0 1492895302.377362: training step 17232 done (global step: 137863) with Loss 0.000758, 0.000758
[1] worker/0 1492895302.410056: training step 17233 done (global step: 137871) with Loss 0.000771, 0.000771
[1] worker/0 1492895302.451053: training step 17234 done (global step: 137879) with Loss 0.000786, 0.000786
0.000786289514585 0.0007862936175
LOSS CONVERGED...at epoch 17235
[1] worker/0 Training complete..!! FINAL LOSS: 0.000786289514585
[1] worker/0 Training ends @ 1492895302.489425
[1] worker/0 Training elapsed time: 562.705231 s
[1] Predicted y_hat[:5]==> [ 0.96463321  0.69937066  0.96949387  0.96573468  0.87774031]
y_hat[:5]===> [ 0.96463321  0.69937066  0.96949387  0.96573468  0.87774031]
y_test[:5]==> [ 0.97753567  0.70309013  0.97447079  0.96657014  0.88810825]
INFO:tensorflow:global_step/sec: 163.099
{'best_itr': 1, 'best_loss': 0.00078628951458508149, '#best_params': [0.001]}
(tensorflow) Srinivasas-Air:nn_dist srianant$


===================================
DISTRIBUTED Worker0/1/2/3 and PS0/1
===================================

[1] worker/1 1492895925.247058: training step 4235 done (global step: 33885) with Loss 0.001892, 0.001892
[1] worker/3 1492895925.278795: training step 4235 done (global step: 33887) with Loss 0.001870, 0.001870
[1] worker/2 1492895925.273438: training step 4235 done (global step: 33887) with Loss 0.001870, 0.001870
[1] worker/0 1492895925.303801: training step 4235 done (global step: 33889) with Loss 0.001837, 0.001837
[1] worker/1 1492895925.351957: training step 4236 done (global step: 33893) with Loss 0.001885, 0.001885
0.00187020422774 0.00187020984877
LOSS CONVERGED...at epoch 4236
[1] worker/2 Training complete..!! FINAL LOSS: 0.00187020422774
[1] worker/2 Training ends @ 1492895925.377287
[1] worker/2 Training elapsed time: 504.472591 s
0.00187020422774 0.00187020984877
LOSS CONVERGED...at epoch 4236
[1] worker/3 Training complete..!! FINAL LOSS: 0.00187020422774
[1] worker/3 Training ends @ 1492895925.379011
[1] worker/3 Training elapsed time: 504.427010 s
[1] Predicted y_hat[:5]==> [ 0.97766795  0.71267078  0.97554962  0.96247383  0.8456245 ]
[1] Predicted y_hat[:5]==> [ 0.97766795  0.71267078  0.97554962  0.96247383  0.8456245 ]
y_hat[:5]===> [ 0.97766795  0.71267078  0.97554962  0.96247383  0.8456245 ]
y_test[:5]==> [ 0.97753567  0.70309013  0.97447079  0.96657014  0.88810825]
y_hat[:5]===> [ 0.97766795  0.71267078  0.97554962  0.96247383  0.8456245 ]
y_test[:5]==> [ 0.97753567  0.70309013  0.97447079  0.96657014  0.88810825]
INFO:tensorflow:global_step/sec: 14.309
INFO:tensorflow:global_step/sec: 0
{'best_itr': 1, '#best_params': [0.001], 'best_loss': 0.001870204227744286}
(tensorflow) Srinivasas-Air:nn_dist srianant$ {'best_itr': 1, '#best_params': [0.001], 'best_loss': 0.001870204227744286}

